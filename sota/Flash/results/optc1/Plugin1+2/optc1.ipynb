{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flash Evaluation on the DARPA OpTC Dataset\n",
    "\n",
    "This notebook is designed for evaluating Flash on the DARPA OpTC dataset. The OpTC dataset is a node-level dataset, crucial for our analysis. Flash is configured to operate in a node-level setting to effectively assess this dataset. The OpTC dataset is enriched with node attributes, making it suitable for running Flash in a decoupled manner. This includes using offline GNN embeddings and a downstream classifier. Our approach tests Flash on this dataset, where Flash generates word2vec embeddings as feature vectors for GNN embeddings. These embeddings are stored in a datastore and used in conjunction with a downstream model for improved detection results.\n",
    "\n",
    "## Accessing the Dataset:\n",
    "- The OpTC dataset can be accessed via this link: [OpTC Dataset](https://drive.google.com/drive/u/0/folders/1n3kkS3KR31KUegn42yk3-e6JkZvf0Caa).\n",
    "- Dataset files for evaluation will be downloaded automatically by the script.\n",
    "- While we provide pre-trained weights, you also have the option to download benign data files for training the models from the ground up.\n",
    "\n",
    "## Data Parsing and Execution:\n",
    "- The script is adept at autonomously parsing the downloaded data files.\n",
    "- For evaluation results, execute all cells in this notebook.\n",
    "\n",
    "## Model Training and Execution Options:\n",
    "- By default, the notebook utilizes pre-trained model weights.\n",
    "- It also offer settings to independently train Graph Neural Networks (GNNs), word2vec, and Xgboost models.\n",
    "- These independently trained models can then be deployed for an evaluation of the system.\n",
    "\n",
    "Following these guidelines will ensure a thorough and effective analysis of the OpTC dataset using Flash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T19:56:38.358378Z",
     "iopub.status.busy": "2025-12-27T19:56:38.358219Z",
     "iopub.status.idle": "2025-12-27T19:56:40.976976Z",
     "shell.execute_reply": "2025-12-27T19:56:40.976355Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import json\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "warnings.filterwarnings('ignore')\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "import csv\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cuda:5\")\n",
    "csv_path = \"../../flash-add-edge/optc-201.csv\"\n",
    "\n",
    "import subprocess\n",
    "gpu_mem = {int(x.split(',')[0]): int(x.split(',')[1]) for x in subprocess.check_output(\n",
    "    [\"nvidia-smi\", \"--query-gpu=index,memory.free\", \"--format=csv,noheader,nounits\"], \n",
    "    encoding='utf-8').strip().split('\\n')}\n",
    "best_gpu = max(gpu_mem.items(), key=lambda x: x[1])[0]\n",
    "device = torch.device(f'cuda:{best_gpu}' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T19:56:41.011897Z",
     "iopub.status.busy": "2025-12-27T19:56:41.010804Z",
     "iopub.status.idle": "2025-12-27T19:56:41.015483Z",
     "shell.execute_reply": "2025-12-27T19:56:41.014977Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gnn_weights = \"trained_weights/optc/gnn_temp.pth\"\n",
    "gnn_weights = \"gnn_temp.pth\"\n",
    "# xgboost_weights = \"trained_weights/optc/xgb.pkl\"\n",
    "xgboost_weights = \"xgb.pkl\"\n",
    "word2vec_weights = 'w2v_optc.model'\n",
    "create_store = True\n",
    "gnnTrain = True\n",
    "xgbTrain = True\n",
    "w2vTrain = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T19:56:41.016907Z",
     "iopub.status.busy": "2025-12-27T19:56:41.016753Z",
     "iopub.status.idle": "2025-12-27T19:56:41.096388Z",
     "shell.execute_reply": "2025-12-27T19:56:41.095616Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import gzip\n",
    "from sklearn.manifold import TSNE\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import xgboost as xgb\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from multiprocessing import Pool\n",
    "from itertools import compress\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T19:56:41.099220Z",
     "iopub.status.busy": "2025-12-27T19:56:41.099062Z",
     "iopub.status.idle": "2025-12-27T19:56:41.102004Z",
     "shell.execute_reply": "2025-12-27T19:56:41.101596Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from multiprocessing import Pool\n",
    "from itertools import compress\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T19:56:41.103368Z",
     "iopub.status.busy": "2025-12-27T19:56:41.103216Z",
     "iopub.status.idle": "2025-12-27T19:56:41.106719Z",
     "shell.execute_reply": "2025-12-27T19:56:41.106314Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import io\n",
    "\n",
    "def extract_logs(filepath, hostid):\n",
    "    search_pattern = f'../SysClient{hostid}'\n",
    "    output_filename = f'../SysClient{hostid}.systemia.com.txt'\n",
    "    \n",
    "    with gzip.open(filepath, 'rt', encoding='utf-8') as fin:\n",
    "        with open(output_filename, 'ab') as f:\n",
    "            out = io.BufferedWriter(f)\n",
    "            for line in fin:\n",
    "                if search_pattern in line:\n",
    "                    out.write(line.encode('utf-8'))\n",
    "            out.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T19:56:41.108103Z",
     "iopub.status.busy": "2025-12-27T19:56:41.107945Z",
     "iopub.status.idle": "2025-12-27T19:56:41.111716Z",
     "shell.execute_reply": "2025-12-27T19:56:41.111307Z"
    }
   },
   "outputs": [],
   "source": [
    "# import gdown\n",
    "from tqdm import tqdm\n",
    "    \n",
    "def prepare_test_set():\n",
    "    # urls = [\n",
    "    #     \"https://drive.google.com/file/d/1HFSyvmgH0jvdnnnTdKfWRjZYOrLWoIkv/view?usp=drive_link\",\n",
    "    #     \"https://drive.google.com/file/d/1pJLxJsDV8sngiedbfVajMetczIgM3PQd/view?usp=drive_link\",\n",
    "    #     \"https://drive.google.com/file/d/1fRQqc68r8-z5BL7H_eAKIDOeHp7okDuM/view?usp=drive_link\",\n",
    "    #     \"https://drive.google.com/file/d/1VfyGr8wfSe8LBIHBWuYBlU8c2CyEgO5C/view?usp=drive_link\",\n",
    "    #     \"https://drive.google.com/file/d/10N9ZPolq_L8HivBqzf_jFKbwjSxddsZp/view?usp=drive_link\",\n",
    "    #     \"https://drive.google.com/file/d/1xIr8gw-4zc8ESjUpYtrFsbOwhPGUSd15/view?usp=drive_link\",\n",
    "    #     \"https://drive.google.com/file/d/1PvlCp2oQaxEBEFGSQWfcFVj19zLOe7yH/view?usp=drive_link\"\n",
    "    # ]\n",
    "\n",
    "    # for url in urls:\n",
    "    #     gdown.download(url, quiet=False, use_cookies=False, fuzzy=True)\n",
    "\n",
    "    log_files = [\n",
    "        (\"AIA-201-225.ecar-2019-12-08T11-05-10.046.json.gz\", \"0201\"),\n",
    "        (\"AIA-201-225.ecar-last.json.gz\", \"0201\"),\n",
    "        (\"AIA-501-525.ecar-2019-11-17T04-01-58.625.json.gz\", \"0501\"),\n",
    "        (\"AIA-501-525.ecar-last.json.gz\", \"0501\"),\n",
    "        (\"AIA-51-75.ecar-last.json.gz\", \"0051\")\n",
    "    ]\n",
    "    \n",
    "    os.system(\"rm SysClient0201.com.txt\")\n",
    "    os.system(\"rm SysClient0501.com.txt\")\n",
    "    os.system(\"rm SysClient0051.com.txt\")\n",
    "\n",
    "    for file, code in tqdm(log_files, desc=\"Extracting logs\", unit=\"file\"):\n",
    "        extract_logs(file, code)\n",
    "\n",
    "# prepare_test_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T19:56:41.113104Z",
     "iopub.status.busy": "2025-12-27T19:56:41.112951Z",
     "iopub.status.idle": "2025-12-27T19:56:41.117056Z",
     "shell.execute_reply": "2025-12-27T19:56:41.116646Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_valid_entry(entry):\n",
    "    valid_objects = {'PROCESS', 'FILE', 'FLOW', 'MODULE'}\n",
    "    invalid_actions = {'START', 'TERMINATE'}\n",
    "\n",
    "    object_valid = entry['object'] in valid_objects\n",
    "    action_valid = entry['action'] not in invalid_actions\n",
    "    actor_object_different = entry['actorID'] != entry['objectID']\n",
    "\n",
    "    return object_valid and action_valid and actor_object_different\n",
    "\n",
    "def Traversal_Rules(data):\n",
    "    filtered_data = {}\n",
    "\n",
    "    for entry in data:\n",
    "        if is_valid_entry(entry):\n",
    "            key = (\n",
    "                entry['action'], \n",
    "                entry['actorID'], \n",
    "                entry['objectID'], \n",
    "                entry['object'], \n",
    "                entry['pid'], \n",
    "                entry['ppid']\n",
    "            )\n",
    "            filtered_data[key] = entry\n",
    "\n",
    "    return list(filtered_data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T19:56:41.118429Z",
     "iopub.status.busy": "2025-12-27T19:56:41.118278Z",
     "iopub.status.idle": "2025-12-27T19:56:41.121805Z",
     "shell.execute_reply": "2025-12-27T19:56:41.121382Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Sentence_Construction(entry):\n",
    "    action = entry[\"action\"]\n",
    "    properties = entry['properties']\n",
    "    object_type = entry['object']\n",
    "\n",
    "    format_strings = {\n",
    "        'PROCESS': \"{parent_image_path} {action} {image_path} {command_line}\",\n",
    "        'FILE': \"{image_path} {action} {file_path}\",\n",
    "        'FLOW': \"{image_path} {action} {src_ip} {src_port} {dest_ip} {dest_port} {direction}\",\n",
    "        'MODULE': \"{image_path} {action} {module_path}\"\n",
    "    }\n",
    "\n",
    "    default_format = \"{image_path} {action} {module_path}\"\n",
    "\n",
    "    try:\n",
    "        format_str = format_strings.get(object_type, default_format)\n",
    "        phrase = format_str.format(action=action, **properties)\n",
    "    except KeyError:\n",
    "        phrase = ''\n",
    "\n",
    "    return phrase.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T19:56:41.123211Z",
     "iopub.status.busy": "2025-12-27T19:56:41.123051Z",
     "iopub.status.idle": "2025-12-27T19:56:41.129060Z",
     "shell.execute_reply": "2025-12-27T19:56:41.128636Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def Extract_Semantic_Info(event):\n",
    "    object_type = event['object']\n",
    "    properties = event['properties']\n",
    "\n",
    "    label_mapping = {\n",
    "        \"PROCESS\": ('parent_image_path', 'image_path'),\n",
    "        \"FILE\": ('image_path', 'file_path'),\n",
    "        \"MODULE\": ('image_path', 'module_path'),\n",
    "        \"FLOW\": ('image_path', 'dest_ip', 'dest_port')\n",
    "    }\n",
    "\n",
    "    label_keys = label_mapping.get(object_type, None)\n",
    "    if label_keys:\n",
    "        labels = [properties.get(key) for key in label_keys]\n",
    "        if all(labels):\n",
    "            event[\"actorname\"], event[\"objectname\"] = labels[0], ' '.join(labels[1:])\n",
    "            return event\n",
    "    return None\n",
    "\n",
    "def transform(text):\n",
    "    labeled_data = [event for event in (Extract_Semantic_Info(x) for x in text) if event]\n",
    "    data = Traversal_Rules(labeled_data)\n",
    "\n",
    "    phrases = [Sentence_Construction(x) for x in data if Sentence_Construction(x)]\n",
    "    for datum, phrase in zip(data, phrases):\n",
    "        datum['phrase'] = phrase\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'].str[:-6], infer_datetime_format=True)\n",
    "    df.sort_values(by='timestamp', inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = [json.loads(line) for line in file]\n",
    "    \n",
    "    return Featurize(transform(content), csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T19:56:41.130538Z",
     "iopub.status.busy": "2025-12-27T19:56:41.130383Z",
     "iopub.status.idle": "2025-12-27T19:56:41.140919Z",
     "shell.execute_reply": "2025-12-27T19:56:41.140436Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def Featurize(df, additional_edges_file=None, thres=1000000000):\n",
    "    dummies = {'PROCESS': 0, 'FLOW': 1, 'FILE': 2, 'MODULE': 3}\n",
    "\n",
    "    nodes = {}\n",
    "    labels = {}\n",
    "    lblmap = {}\n",
    "    neimap = {}\n",
    "    edges = []\n",
    "    for index, row in df.iterrows():\n",
    "        actor_id, object_id = row['actorID'], row[\"objectID\"]\n",
    "        object_type = row['object']\n",
    "\n",
    "        nodes.setdefault(actor_id, []).extend(row['phrase'])\n",
    "        nodes.setdefault(object_id, []).extend(row['phrase'])\n",
    "\n",
    "        labels[actor_id] = dummies.get('PROCESS', -1)\n",
    "        labels[object_id] = dummies.get(object_type, -1)\n",
    "\n",
    "        lblmap[actor_id] = row['actorname']\n",
    "        lblmap[object_id] = row['objectname']\n",
    "\n",
    "        neimap.setdefault(actor_id, set()).add(row['objectname'])\n",
    "        neimap.setdefault(object_id, set()).add(row['actorname'])\n",
    "\n",
    "        edge_type = row['properties']['direction'] if object_type == 'FLOW' else row['action']\n",
    "        edges.append((actor_id, object_id, edge_type))\n",
    "\n",
    "    if additional_edges_file:\n",
    "        try:\n",
    "            degrees = defaultdict(int)\n",
    "            for src_id, dst_id, _ in edges:\n",
    "                degrees[src_id] += 1\n",
    "                degrees[dst_id] += 1\n",
    "                \n",
    "            all_degrees = list(degrees.values())\n",
    "            all_degrees.sort(reverse=True)\n",
    "            print(all_degrees[:3000])\n",
    "\n",
    "            additional_edges_df = pd.read_csv(additional_edges_file, header=None, names=['src', 'dst'])\n",
    "            \n",
    "            count1, count2, count3 = 0, 0, 0\n",
    "            for _, row in additional_edges_df.iterrows():\n",
    "                src_id = str(row['src']).strip()\n",
    "                dst_id = str(row['dst']).strip()\n",
    "\n",
    "                if src_id in nodes and dst_id in nodes:\n",
    "                    if degrees[src_id] <= thres and degrees[dst_id] <= thres:     \n",
    "                        edges.append((src_id, dst_id, 'ADDITIONAL_EDGE'))\n",
    "                        count1 += 1\n",
    "                    else:\n",
    "                        count2 += 1\n",
    "                else:\n",
    "                    count3 += 1\n",
    "                \n",
    "            print(f'Sucessfully add edges: {count1}\\tPrune edges: {count2}\\tFail to add edges: {count3}')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"读取额外边文件时出错: {e}\")\n",
    "\n",
    "\n",
    "    features, feat_labels, edge_index = [], [], [[], []]\n",
    "    node_index = {}\n",
    "\n",
    "    \n",
    "    for node, phrases in nodes.items():\n",
    "        if not (len(phrases) == 1 and phrases[0] == 'DELETE'):\n",
    "            features.append(infer(phrases))\n",
    "            feat_labels.append(labels[node])\n",
    "            node_index[node] = len(features) - 1\n",
    "\n",
    "    \n",
    "    for src, dst, _ in edges:\n",
    "        if src in node_index and dst in node_index:\n",
    "            edge_index[0].append(node_index[src])\n",
    "            edge_index[1].append(node_index[dst])\n",
    "\n",
    "    mapp = list(node_index.keys())\n",
    "\n",
    "    return features, np.array(feat_labels), edge_index, mapp, lblmap, neimap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T19:56:41.142697Z",
     "iopub.status.busy": "2025-12-27T19:56:41.142542Z",
     "iopub.status.idle": "2025-12-27T19:56:41.145516Z",
     "shell.execute_reply": "2025-12-27T19:56:41.145110Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "class EpochSaver(CallbackAny2Vec):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        model.save('w2v_optc.model')\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T19:56:41.146967Z",
     "iopub.status.busy": "2025-12-27T19:56:41.146805Z",
     "iopub.status.idle": "2025-12-27T19:56:41.149913Z",
     "shell.execute_reply": "2025-12-27T19:56:41.149510Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EpochLogger(CallbackAny2Vec):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(\"Epoch #{} start\".format(self.epoch))\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print(\"Epoch #{} end\".format(self.epoch))\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T19:56:41.151297Z",
     "iopub.status.busy": "2025-12-27T19:56:41.151144Z",
     "iopub.status.idle": "2025-12-27T19:56:41.155543Z",
     "shell.execute_reply": "2025-12-27T19:56:41.155133Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def prepare_sentences(df):\n",
    "    nodes = {}\n",
    "    for index, row in df.iterrows():\n",
    "        for key in ['actorID', 'objectID']:\n",
    "            node_id = row[key]\n",
    "            nodes.setdefault(node_id, []).extend(row['phrase'])\n",
    "    return list(nodes.values())\n",
    "\n",
    "def train_word2vec_model(train_file_path):\n",
    "    with open(train_file_path, 'r') as file:\n",
    "        content = [json.loads(line) for line in file]\n",
    "\n",
    "    events = transform(content)\n",
    "    phrases = prepare_sentences(events)\n",
    "\n",
    "    logger = EpochLogger()\n",
    "    saver = EpochSaver()\n",
    "    # word2vec = Word2Vec(sentences=phrases, vector_size=20, window=5, min_count=1, workers=8, epochs=300, callbacks=[saver, logger])\n",
    "    word2vec = Word2Vec(sentences=phrases, vector_size=10, window=5, min_count=1, workers=8, epochs=300, callbacks=[saver, logger])\n",
    "\n",
    "    return word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T19:56:41.159147Z",
     "iopub.status.busy": "2025-12-27T19:56:41.158989Z",
     "iopub.status.idle": "2025-12-27T19:59:41.651721Z",
     "shell.execute_reply": "2025-12-27T19:59:41.650997Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 start\n",
      "Epoch #0 end\n",
      "Epoch #1 start\n",
      "Epoch #1 end\n",
      "Epoch #2 start\n",
      "Epoch #2 end\n",
      "Epoch #3 start\n",
      "Epoch #3 end\n",
      "Epoch #4 start\n",
      "Epoch #4 end\n",
      "Epoch #5 start\n",
      "Epoch #5 end\n",
      "Epoch #6 start\n",
      "Epoch #6 end\n",
      "Epoch #7 start\n",
      "Epoch #7 end\n",
      "Epoch #8 start\n",
      "Epoch #8 end\n",
      "Epoch #9 start\n",
      "Epoch #9 end\n",
      "Epoch #10 start\n",
      "Epoch #10 end\n",
      "Epoch #11 start\n",
      "Epoch #11 end\n",
      "Epoch #12 start\n",
      "Epoch #12 end\n",
      "Epoch #13 start\n",
      "Epoch #13 end\n",
      "Epoch #14 start\n",
      "Epoch #14 end\n",
      "Epoch #15 start\n",
      "Epoch #15 end\n",
      "Epoch #16 start\n",
      "Epoch #16 end\n",
      "Epoch #17 start\n",
      "Epoch #17 end\n",
      "Epoch #18 start\n",
      "Epoch #18 end\n",
      "Epoch #19 start\n",
      "Epoch #19 end\n",
      "Epoch #20 start\n",
      "Epoch #20 end\n",
      "Epoch #21 start\n",
      "Epoch #21 end\n",
      "Epoch #22 start\n",
      "Epoch #22 end\n",
      "Epoch #23 start\n",
      "Epoch #23 end\n",
      "Epoch #24 start\n",
      "Epoch #24 end\n",
      "Epoch #25 start\n",
      "Epoch #25 end\n",
      "Epoch #26 start\n",
      "Epoch #26 end\n",
      "Epoch #27 start\n",
      "Epoch #27 end\n",
      "Epoch #28 start\n",
      "Epoch #28 end\n",
      "Epoch #29 start\n",
      "Epoch #29 end\n",
      "Epoch #30 start\n",
      "Epoch #30 end\n",
      "Epoch #31 start\n",
      "Epoch #31 end\n",
      "Epoch #32 start\n",
      "Epoch #32 end\n",
      "Epoch #33 start\n",
      "Epoch #33 end\n",
      "Epoch #34 start\n",
      "Epoch #34 end\n",
      "Epoch #35 start\n",
      "Epoch #35 end\n",
      "Epoch #36 start\n",
      "Epoch #36 end\n",
      "Epoch #37 start\n",
      "Epoch #37 end\n",
      "Epoch #38 start\n",
      "Epoch #38 end\n",
      "Epoch #39 start\n",
      "Epoch #39 end\n",
      "Epoch #40 start\n",
      "Epoch #40 end\n",
      "Epoch #41 start\n",
      "Epoch #41 end\n",
      "Epoch #42 start\n",
      "Epoch #42 end\n",
      "Epoch #43 start\n",
      "Epoch #43 end\n",
      "Epoch #44 start\n",
      "Epoch #44 end\n",
      "Epoch #45 start\n",
      "Epoch #45 end\n",
      "Epoch #46 start\n",
      "Epoch #46 end\n",
      "Epoch #47 start\n",
      "Epoch #47 end\n",
      "Epoch #48 start\n",
      "Epoch #48 end\n",
      "Epoch #49 start\n",
      "Epoch #49 end\n",
      "Epoch #50 start\n",
      "Epoch #50 end\n",
      "Epoch #51 start\n",
      "Epoch #51 end\n",
      "Epoch #52 start\n",
      "Epoch #52 end\n",
      "Epoch #53 start\n",
      "Epoch #53 end\n",
      "Epoch #54 start\n",
      "Epoch #54 end\n",
      "Epoch #55 start\n",
      "Epoch #55 end\n",
      "Epoch #56 start\n",
      "Epoch #56 end\n",
      "Epoch #57 start\n",
      "Epoch #57 end\n",
      "Epoch #58 start\n",
      "Epoch #58 end\n",
      "Epoch #59 start\n",
      "Epoch #59 end\n",
      "Epoch #60 start\n",
      "Epoch #60 end\n",
      "Epoch #61 start\n",
      "Epoch #61 end\n",
      "Epoch #62 start\n",
      "Epoch #62 end\n",
      "Epoch #63 start\n",
      "Epoch #63 end\n",
      "Epoch #64 start\n",
      "Epoch #64 end\n",
      "Epoch #65 start\n",
      "Epoch #65 end\n",
      "Epoch #66 start\n",
      "Epoch #66 end\n",
      "Epoch #67 start\n",
      "Epoch #67 end\n",
      "Epoch #68 start\n",
      "Epoch #68 end\n",
      "Epoch #69 start\n",
      "Epoch #69 end\n",
      "Epoch #70 start\n",
      "Epoch #70 end\n",
      "Epoch #71 start\n",
      "Epoch #71 end\n",
      "Epoch #72 start\n",
      "Epoch #72 end\n",
      "Epoch #73 start\n",
      "Epoch #73 end\n",
      "Epoch #74 start\n",
      "Epoch #74 end\n",
      "Epoch #75 start\n",
      "Epoch #75 end\n",
      "Epoch #76 start\n",
      "Epoch #76 end\n",
      "Epoch #77 start\n",
      "Epoch #77 end\n",
      "Epoch #78 start\n",
      "Epoch #78 end\n",
      "Epoch #79 start\n",
      "Epoch #79 end\n",
      "Epoch #80 start\n",
      "Epoch #80 end\n",
      "Epoch #81 start\n",
      "Epoch #81 end\n",
      "Epoch #82 start\n",
      "Epoch #82 end\n",
      "Epoch #83 start\n",
      "Epoch #83 end\n",
      "Epoch #84 start\n",
      "Epoch #84 end\n",
      "Epoch #85 start\n",
      "Epoch #85 end\n",
      "Epoch #86 start\n",
      "Epoch #86 end\n",
      "Epoch #87 start\n",
      "Epoch #87 end\n",
      "Epoch #88 start\n",
      "Epoch #88 end\n",
      "Epoch #89 start\n",
      "Epoch #89 end\n",
      "Epoch #90 start\n",
      "Epoch #90 end\n",
      "Epoch #91 start\n",
      "Epoch #91 end\n",
      "Epoch #92 start\n",
      "Epoch #92 end\n",
      "Epoch #93 start\n",
      "Epoch #93 end\n",
      "Epoch #94 start\n",
      "Epoch #94 end\n",
      "Epoch #95 start\n",
      "Epoch #95 end\n",
      "Epoch #96 start\n",
      "Epoch #96 end\n",
      "Epoch #97 start\n",
      "Epoch #97 end\n",
      "Epoch #98 start\n",
      "Epoch #98 end\n",
      "Epoch #99 start\n",
      "Epoch #99 end\n",
      "Epoch #100 start\n",
      "Epoch #100 end\n",
      "Epoch #101 start\n",
      "Epoch #101 end\n",
      "Epoch #102 start\n",
      "Epoch #102 end\n",
      "Epoch #103 start\n",
      "Epoch #103 end\n",
      "Epoch #104 start\n",
      "Epoch #104 end\n",
      "Epoch #105 start\n",
      "Epoch #105 end\n",
      "Epoch #106 start\n",
      "Epoch #106 end\n",
      "Epoch #107 start\n",
      "Epoch #107 end\n",
      "Epoch #108 start\n",
      "Epoch #108 end\n",
      "Epoch #109 start\n",
      "Epoch #109 end\n",
      "Epoch #110 start\n",
      "Epoch #110 end\n",
      "Epoch #111 start\n",
      "Epoch #111 end\n",
      "Epoch #112 start\n",
      "Epoch #112 end\n",
      "Epoch #113 start\n",
      "Epoch #113 end\n",
      "Epoch #114 start\n",
      "Epoch #114 end\n",
      "Epoch #115 start\n",
      "Epoch #115 end\n",
      "Epoch #116 start\n",
      "Epoch #116 end\n",
      "Epoch #117 start\n",
      "Epoch #117 end\n",
      "Epoch #118 start\n",
      "Epoch #118 end\n",
      "Epoch #119 start\n",
      "Epoch #119 end\n",
      "Epoch #120 start\n",
      "Epoch #120 end\n",
      "Epoch #121 start\n",
      "Epoch #121 end\n",
      "Epoch #122 start\n",
      "Epoch #122 end\n",
      "Epoch #123 start\n",
      "Epoch #123 end\n",
      "Epoch #124 start\n",
      "Epoch #124 end\n",
      "Epoch #125 start\n",
      "Epoch #125 end\n",
      "Epoch #126 start\n",
      "Epoch #126 end\n",
      "Epoch #127 start\n",
      "Epoch #127 end\n",
      "Epoch #128 start\n",
      "Epoch #128 end\n",
      "Epoch #129 start\n",
      "Epoch #129 end\n",
      "Epoch #130 start\n",
      "Epoch #130 end\n",
      "Epoch #131 start\n",
      "Epoch #131 end\n",
      "Epoch #132 start\n",
      "Epoch #132 end\n",
      "Epoch #133 start\n",
      "Epoch #133 end\n",
      "Epoch #134 start\n",
      "Epoch #134 end\n",
      "Epoch #135 start\n",
      "Epoch #135 end\n",
      "Epoch #136 start\n",
      "Epoch #136 end\n",
      "Epoch #137 start\n",
      "Epoch #137 end\n",
      "Epoch #138 start\n",
      "Epoch #138 end\n",
      "Epoch #139 start\n",
      "Epoch #139 end\n",
      "Epoch #140 start\n",
      "Epoch #140 end\n",
      "Epoch #141 start\n",
      "Epoch #141 end\n",
      "Epoch #142 start\n",
      "Epoch #142 end\n",
      "Epoch #143 start\n",
      "Epoch #143 end\n",
      "Epoch #144 start\n",
      "Epoch #144 end\n",
      "Epoch #145 start\n",
      "Epoch #145 end\n",
      "Epoch #146 start\n",
      "Epoch #146 end\n",
      "Epoch #147 start\n",
      "Epoch #147 end\n",
      "Epoch #148 start\n",
      "Epoch #148 end\n",
      "Epoch #149 start\n",
      "Epoch #149 end\n",
      "Epoch #150 start\n",
      "Epoch #150 end\n",
      "Epoch #151 start\n",
      "Epoch #151 end\n",
      "Epoch #152 start\n",
      "Epoch #152 end\n",
      "Epoch #153 start\n",
      "Epoch #153 end\n",
      "Epoch #154 start\n",
      "Epoch #154 end\n",
      "Epoch #155 start\n",
      "Epoch #155 end\n",
      "Epoch #156 start\n",
      "Epoch #156 end\n",
      "Epoch #157 start\n",
      "Epoch #157 end\n",
      "Epoch #158 start\n",
      "Epoch #158 end\n",
      "Epoch #159 start\n",
      "Epoch #159 end\n",
      "Epoch #160 start\n",
      "Epoch #160 end\n",
      "Epoch #161 start\n",
      "Epoch #161 end\n",
      "Epoch #162 start\n",
      "Epoch #162 end\n",
      "Epoch #163 start\n",
      "Epoch #163 end\n",
      "Epoch #164 start\n",
      "Epoch #164 end\n",
      "Epoch #165 start\n",
      "Epoch #165 end\n",
      "Epoch #166 start\n",
      "Epoch #166 end\n",
      "Epoch #167 start\n",
      "Epoch #167 end\n",
      "Epoch #168 start\n",
      "Epoch #168 end\n",
      "Epoch #169 start\n",
      "Epoch #169 end\n",
      "Epoch #170 start\n",
      "Epoch #170 end\n",
      "Epoch #171 start\n",
      "Epoch #171 end\n",
      "Epoch #172 start\n",
      "Epoch #172 end\n",
      "Epoch #173 start\n",
      "Epoch #173 end\n",
      "Epoch #174 start\n",
      "Epoch #174 end\n",
      "Epoch #175 start\n",
      "Epoch #175 end\n",
      "Epoch #176 start\n",
      "Epoch #176 end\n",
      "Epoch #177 start\n",
      "Epoch #177 end\n",
      "Epoch #178 start\n",
      "Epoch #178 end\n",
      "Epoch #179 start\n",
      "Epoch #179 end\n",
      "Epoch #180 start\n",
      "Epoch #180 end\n",
      "Epoch #181 start\n",
      "Epoch #181 end\n",
      "Epoch #182 start\n",
      "Epoch #182 end\n",
      "Epoch #183 start\n",
      "Epoch #183 end\n",
      "Epoch #184 start\n",
      "Epoch #184 end\n",
      "Epoch #185 start\n",
      "Epoch #185 end\n",
      "Epoch #186 start\n",
      "Epoch #186 end\n",
      "Epoch #187 start\n",
      "Epoch #187 end\n",
      "Epoch #188 start\n",
      "Epoch #188 end\n",
      "Epoch #189 start\n",
      "Epoch #189 end\n",
      "Epoch #190 start\n",
      "Epoch #190 end\n",
      "Epoch #191 start\n",
      "Epoch #191 end\n",
      "Epoch #192 start\n",
      "Epoch #192 end\n",
      "Epoch #193 start\n",
      "Epoch #193 end\n",
      "Epoch #194 start\n",
      "Epoch #194 end\n",
      "Epoch #195 start\n",
      "Epoch #195 end\n",
      "Epoch #196 start\n",
      "Epoch #196 end\n",
      "Epoch #197 start\n",
      "Epoch #197 end\n",
      "Epoch #198 start\n",
      "Epoch #198 end\n",
      "Epoch #199 start\n",
      "Epoch #199 end\n",
      "Epoch #200 start\n",
      "Epoch #200 end\n",
      "Epoch #201 start\n",
      "Epoch #201 end\n",
      "Epoch #202 start\n",
      "Epoch #202 end\n",
      "Epoch #203 start\n",
      "Epoch #203 end\n",
      "Epoch #204 start\n",
      "Epoch #204 end\n",
      "Epoch #205 start\n",
      "Epoch #205 end\n",
      "Epoch #206 start\n",
      "Epoch #206 end\n",
      "Epoch #207 start\n",
      "Epoch #207 end\n",
      "Epoch #208 start\n",
      "Epoch #208 end\n",
      "Epoch #209 start\n",
      "Epoch #209 end\n",
      "Epoch #210 start\n",
      "Epoch #210 end\n",
      "Epoch #211 start\n",
      "Epoch #211 end\n",
      "Epoch #212 start\n",
      "Epoch #212 end\n",
      "Epoch #213 start\n",
      "Epoch #213 end\n",
      "Epoch #214 start\n",
      "Epoch #214 end\n",
      "Epoch #215 start\n",
      "Epoch #215 end\n",
      "Epoch #216 start\n",
      "Epoch #216 end\n",
      "Epoch #217 start\n",
      "Epoch #217 end\n",
      "Epoch #218 start\n",
      "Epoch #218 end\n",
      "Epoch #219 start\n",
      "Epoch #219 end\n",
      "Epoch #220 start\n",
      "Epoch #220 end\n",
      "Epoch #221 start\n",
      "Epoch #221 end\n",
      "Epoch #222 start\n",
      "Epoch #222 end\n",
      "Epoch #223 start\n",
      "Epoch #223 end\n",
      "Epoch #224 start\n",
      "Epoch #224 end\n",
      "Epoch #225 start\n",
      "Epoch #225 end\n",
      "Epoch #226 start\n",
      "Epoch #226 end\n",
      "Epoch #227 start\n",
      "Epoch #227 end\n",
      "Epoch #228 start\n",
      "Epoch #228 end\n",
      "Epoch #229 start\n",
      "Epoch #229 end\n",
      "Epoch #230 start\n",
      "Epoch #230 end\n",
      "Epoch #231 start\n",
      "Epoch #231 end\n",
      "Epoch #232 start\n",
      "Epoch #232 end\n",
      "Epoch #233 start\n",
      "Epoch #233 end\n",
      "Epoch #234 start\n",
      "Epoch #234 end\n",
      "Epoch #235 start\n",
      "Epoch #235 end\n",
      "Epoch #236 start\n",
      "Epoch #236 end\n",
      "Epoch #237 start\n",
      "Epoch #237 end\n",
      "Epoch #238 start\n",
      "Epoch #238 end\n",
      "Epoch #239 start\n",
      "Epoch #239 end\n",
      "Epoch #240 start\n",
      "Epoch #240 end\n",
      "Epoch #241 start\n",
      "Epoch #241 end\n",
      "Epoch #242 start\n",
      "Epoch #242 end\n",
      "Epoch #243 start\n",
      "Epoch #243 end\n",
      "Epoch #244 start\n",
      "Epoch #244 end\n",
      "Epoch #245 start\n",
      "Epoch #245 end\n",
      "Epoch #246 start\n",
      "Epoch #246 end\n",
      "Epoch #247 start\n",
      "Epoch #247 end\n",
      "Epoch #248 start\n",
      "Epoch #248 end\n",
      "Epoch #249 start\n",
      "Epoch #249 end\n",
      "Epoch #250 start\n",
      "Epoch #250 end\n",
      "Epoch #251 start\n",
      "Epoch #251 end\n",
      "Epoch #252 start\n",
      "Epoch #252 end\n",
      "Epoch #253 start\n",
      "Epoch #253 end\n",
      "Epoch #254 start\n",
      "Epoch #254 end\n",
      "Epoch #255 start\n",
      "Epoch #255 end\n",
      "Epoch #256 start\n",
      "Epoch #256 end\n",
      "Epoch #257 start\n",
      "Epoch #257 end\n",
      "Epoch #258 start\n",
      "Epoch #258 end\n",
      "Epoch #259 start\n",
      "Epoch #259 end\n",
      "Epoch #260 start\n",
      "Epoch #260 end\n",
      "Epoch #261 start\n",
      "Epoch #261 end\n",
      "Epoch #262 start\n",
      "Epoch #262 end\n",
      "Epoch #263 start\n",
      "Epoch #263 end\n",
      "Epoch #264 start\n",
      "Epoch #264 end\n",
      "Epoch #265 start\n",
      "Epoch #265 end\n",
      "Epoch #266 start\n",
      "Epoch #266 end\n",
      "Epoch #267 start\n",
      "Epoch #267 end\n",
      "Epoch #268 start\n",
      "Epoch #268 end\n",
      "Epoch #269 start\n",
      "Epoch #269 end\n",
      "Epoch #270 start\n",
      "Epoch #270 end\n",
      "Epoch #271 start\n",
      "Epoch #271 end\n",
      "Epoch #272 start\n",
      "Epoch #272 end\n",
      "Epoch #273 start\n",
      "Epoch #273 end\n",
      "Epoch #274 start\n",
      "Epoch #274 end\n",
      "Epoch #275 start\n",
      "Epoch #275 end\n",
      "Epoch #276 start\n",
      "Epoch #276 end\n",
      "Epoch #277 start\n",
      "Epoch #277 end\n",
      "Epoch #278 start\n",
      "Epoch #278 end\n",
      "Epoch #279 start\n",
      "Epoch #279 end\n",
      "Epoch #280 start\n",
      "Epoch #280 end\n",
      "Epoch #281 start\n",
      "Epoch #281 end\n",
      "Epoch #282 start\n",
      "Epoch #282 end\n",
      "Epoch #283 start\n",
      "Epoch #283 end\n",
      "Epoch #284 start\n",
      "Epoch #284 end\n",
      "Epoch #285 start\n",
      "Epoch #285 end\n",
      "Epoch #286 start\n",
      "Epoch #286 end\n",
      "Epoch #287 start\n",
      "Epoch #287 end\n",
      "Epoch #288 start\n",
      "Epoch #288 end\n",
      "Epoch #289 start\n",
      "Epoch #289 end\n",
      "Epoch #290 start\n",
      "Epoch #290 end\n",
      "Epoch #291 start\n",
      "Epoch #291 end\n",
      "Epoch #292 start\n",
      "Epoch #292 end\n",
      "Epoch #293 start\n",
      "Epoch #293 end\n",
      "Epoch #294 start\n",
      "Epoch #294 end\n",
      "Epoch #295 start\n",
      "Epoch #295 end\n",
      "Epoch #296 start\n",
      "Epoch #296 end\n",
      "Epoch #297 start\n",
      "Epoch #297 end\n",
      "Epoch #298 start\n",
      "Epoch #298 end\n",
      "Epoch #299 start\n",
      "Epoch #299 end\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "class PositionalEncoder:\n",
    "\n",
    "    def __init__(self, d_model, max_len=100000):\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        self.pe = torch.zeros(max_len, d_model)\n",
    "        self.pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    def embed(self, x):\n",
    "        return x + self.pe[:x.size(0)]\n",
    "\n",
    "\n",
    "def infer(document):\n",
    "    word_embeddings = [w2vmodel.wv[word] for word in document if word in w2vmodel.wv]\n",
    "    \n",
    "    if not word_embeddings:\n",
    "        return np.zeros(10)\n",
    "    \n",
    "    output_embedding = torch.tensor(word_embeddings, dtype=torch.float)\n",
    "    if len(document) < 100000:\n",
    "        output_embedding = encoder.embed(output_embedding)\n",
    "\n",
    "    output_embedding = output_embedding.detach().cpu().numpy()\n",
    "    \n",
    "    mean_embedding = np.mean(output_embedding, axis=0)\n",
    "    \n",
    "    return mean_embedding\n",
    "\n",
    "# encoder = PositionalEncoder(20)\n",
    "encoder = PositionalEncoder(10)\n",
    "if w2vTrain:\n",
    "    file_path = '../SysClient0201.systemia.com.txt'\n",
    "    # 重新训练w2v\n",
    "    w2vmodel = train_word2vec_model(file_path)\n",
    "else:\n",
    "    w2vmodel = Word2Vec.load(word2vec_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T19:59:41.654769Z",
     "iopub.status.busy": "2025-12-27T19:59:41.654578Z",
     "iopub.status.idle": "2025-12-27T19:59:41.703496Z",
     "shell.execute_reply": "2025-12-27T19:59:41.702780Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        # self.conv1 = SAGEConv(20, 32, normalize=True)\n",
    "        self.conv1 = SAGEConv(10, 32, normalize=True)\n",
    "        self.conv2 = SAGEConv(32, 20, normalize=True)\n",
    "        self.linear = nn.Linear(in_features=20, out_features=4)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "    \n",
    "        x = self.encode(x, edge_index)\n",
    "        x = self.linear(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def encode(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T19:59:41.706971Z",
     "iopub.status.busy": "2025-12-27T19:59:41.706532Z",
     "iopub.status.idle": "2025-12-27T19:59:42.514555Z",
     "shell.execute_reply": "2025-12-27T19:59:42.513691Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "model = GCN().to(device)\n",
    "if not gnnTrain:\n",
    "    model.load_state_dict(torch.load(gnn_weights))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T19:59:42.518027Z",
     "iopub.status.busy": "2025-12-27T19:59:42.517845Z",
     "iopub.status.idle": "2025-12-27T20:00:25.515396Z",
     "shell.execute_reply": "2025-12-27T20:00:25.514671Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72582, 8448, 6447, 4648, 3292, 2731, 2334, 2326, 2109, 2079, 1385, 1384, 1384, 1384, 1383, 1124, 1124, 1120, 1120, 1120, 1119, 1045, 1021, 1014, 893, 814, 808, 788, 767, 751, 717, 666, 650, 646, 643, 622, 607, 606, 606, 606, 606, 606, 606, 606, 606, 605, 604, 603, 603, 594, 556, 546, 534, 519, 519, 515, 498, 494, 488, 485, 480, 465, 451, 441, 441, 441, 441, 441, 441, 441, 441, 441, 441, 441, 441, 441, 440, 437, 419, 412, 411, 410, 406, 404, 396, 394, 388, 367, 365, 364, 361, 354, 354, 353, 333, 326, 313, 307, 291, 288, 280, 279, 276, 271, 271, 270, 264, 262, 256, 256, 255, 255, 255, 255, 255, 254, 254, 254, 254, 254, 254, 243, 237, 235, 230, 225, 225, 223, 221, 221, 220, 220, 219, 218, 217, 217, 215, 215, 215, 214, 213, 205, 205, 205, 203, 198, 198, 196, 196, 196, 196, 196, 196, 196, 196, 196, 194, 192, 191, 182, 182, 180, 176, 176, 174, 171, 170, 169, 168, 167, 166, 165, 165, 165, 164, 164, 161, 161, 159, 158, 154, 154, 152, 152, 151, 151, 151, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 149, 147, 146, 144, 144, 143, 142, 142, 141, 140, 140, 139, 139, 139, 138, 136, 135, 134, 134, 134, 134, 134, 134, 133, 133, 131, 131, 131, 130, 130, 129, 128, 128, 128, 128, 127, 127, 127, 127, 127, 125, 124, 124, 123, 123, 122, 121, 121, 121, 120, 119, 119, 119, 118, 118, 118, 118, 118, 118, 117, 117, 117, 117, 117, 117, 117, 116, 116, 116, 115, 113, 113, 112, 112, 111, 111, 110, 110, 109, 108, 108, 108, 107, 105, 105, 105, 104, 103, 103, 102, 102, 101, 101, 100, 100, 98, 98, 96, 96, 96, 95, 95, 94, 94, 94, 93, 93, 93, 93, 93, 93, 93, 93, 93, 92, 92, 92, 90, 90, 90, 90, 90, 89, 89, 89, 88, 88, 88, 87, 87, 85, 85, 85, 84, 84, 84, 84, 84, 84, 84, 83, 82, 82, 82, 81, 81, 81, 81, 81, 81, 81, 80, 80, 80, 79, 79, 79, 78, 78, 78, 78, 78, 78, 77, 77, 76, 76, 76, 75, 75, 75, 75, 75, 74, 74, 74, 74, 74, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 70, 70, 70, 69, 69, 69, 69, 69, 68, 68, 68, 68, 68, 68, 68, 67, 67, 67, 67, 67, 67, 67, 67, 67, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]\n",
      "Sucessfully add edges: 194641\tPrune edges: 0\tFail to add edges: 143231\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "\n",
    "if gnnTrain or create_store:\n",
    "    file_path = '../SysClient0201.systemia.com.txt'\n",
    "    nodes,labels,edges,mapp,lbl,nemap = load_data(file_path)\n",
    "\n",
    "    ngram_class_weights = [\n",
    "        0.7667,  \n",
    "        1.2574,  \n",
    "        1.5000,  \n",
    "        0.5000 \n",
    "    ]\n",
    "    class_weights = torch.tensor(ngram_class_weights,dtype=torch.float).to(device)\n",
    "    \n",
    "    criterion = CrossEntropyLoss(weight=class_weights,reduction='mean')\n",
    "\n",
    "    graph = Data(x=torch.tensor(nodes,dtype=torch.float).to(device),y=torch.tensor(labels,dtype=torch.long).to(device), edge_index=torch.tensor(edges,dtype=torch.long).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T20:00:25.519067Z",
     "iopub.status.busy": "2025-12-27T20:00:25.518879Z",
     "iopub.status.idle": "2025-12-27T20:00:36.561101Z",
     "shell.execute_reply": "2025-12-27T20:00:36.560137Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0. Training Loss: 0.00013, Accuracy: 0.82508\n",
      "Epoch #1. Training Loss: 0.00010, Accuracy: 0.93711\n",
      "Epoch #2. Training Loss: 0.00009, Accuracy: 0.95051\n",
      "Epoch #3. Training Loss: 0.00008, Accuracy: 0.95363\n",
      "Epoch #4. Training Loss: 0.00008, Accuracy: 0.95421\n",
      "Epoch #5. Training Loss: 0.00008, Accuracy: 0.95589\n",
      "Epoch #6. Training Loss: 0.00008, Accuracy: 0.95520\n",
      "Epoch #7. Training Loss: 0.00008, Accuracy: 0.95588\n",
      "Epoch #8. Training Loss: 0.00008, Accuracy: 0.95671\n",
      "Epoch #9. Training Loss: 0.00008, Accuracy: 0.95703\n",
      "Epoch #10. Training Loss: 0.00008, Accuracy: 0.95378\n",
      "Epoch #11. Training Loss: 0.00008, Accuracy: 0.95518\n",
      "Epoch #12. Training Loss: 0.00008, Accuracy: 0.95672\n",
      "Epoch #13. Training Loss: 0.00008, Accuracy: 0.95706\n",
      "Epoch #14. Training Loss: 0.00008, Accuracy: 0.95686\n",
      "Epoch #15. Training Loss: 0.00008, Accuracy: 0.95723\n",
      "Epoch #16. Training Loss: 0.00008, Accuracy: 0.95689\n",
      "Epoch #17. Training Loss: 0.00008, Accuracy: 0.95572\n",
      "Epoch #18. Training Loss: 0.00008, Accuracy: 0.95699\n",
      "Epoch #19. Training Loss: 0.00008, Accuracy: 0.95736\n",
      "Epoch #20. Training Loss: 0.00008, Accuracy: 0.95736\n",
      "Epoch #21. Training Loss: 0.00008, Accuracy: 0.95662\n",
      "Epoch #22. Training Loss: 0.00008, Accuracy: 0.95713\n",
      "Epoch #23. Training Loss: 0.00008, Accuracy: 0.95671\n",
      "Epoch #24. Training Loss: 0.00008, Accuracy: 0.95714\n",
      "Epoch #25. Training Loss: 0.00008, Accuracy: 0.95701\n",
      "Epoch #26. Training Loss: 0.00008, Accuracy: 0.95729\n",
      "Epoch #27. Training Loss: 0.00008, Accuracy: 0.95650\n",
      "Epoch #28. Training Loss: 0.00008, Accuracy: 0.95728\n",
      "Epoch #29. Training Loss: 0.00008, Accuracy: 0.95577\n",
      "Epoch #30. Training Loss: 0.00008, Accuracy: 0.95618\n",
      "Epoch #31. Training Loss: 0.00008, Accuracy: 0.95718\n",
      "Epoch #32. Training Loss: 0.00008, Accuracy: 0.95755\n",
      "Epoch #33. Training Loss: 0.00008, Accuracy: 0.95617\n",
      "Epoch #34. Training Loss: 0.00008, Accuracy: 0.95653\n",
      "Epoch #35. Training Loss: 0.00008, Accuracy: 0.95725\n",
      "Epoch #36. Training Loss: 0.00008, Accuracy: 0.95689\n",
      "Epoch #37. Training Loss: 0.00008, Accuracy: 0.95748\n",
      "Epoch #38. Training Loss: 0.00008, Accuracy: 0.95700\n",
      "Epoch #39. Training Loss: 0.00008, Accuracy: 0.95748\n",
      "Epoch #40. Training Loss: 0.00008, Accuracy: 0.95690\n",
      "Epoch #41. Training Loss: 0.00008, Accuracy: 0.95765\n",
      "Epoch #42. Training Loss: 0.00008, Accuracy: 0.95719\n",
      "Epoch #43. Training Loss: 0.00008, Accuracy: 0.95690\n",
      "Epoch #44. Training Loss: 0.00008, Accuracy: 0.95703\n",
      "Epoch #45. Training Loss: 0.00008, Accuracy: 0.95612\n",
      "Epoch #46. Training Loss: 0.00008, Accuracy: 0.95531\n",
      "Epoch #47. Training Loss: 0.00008, Accuracy: 0.95703\n",
      "Epoch #48. Training Loss: 0.00008, Accuracy: 0.95718\n",
      "Epoch #49. Training Loss: 0.00008, Accuracy: 0.95675\n",
      "Epoch #50. Training Loss: 0.00008, Accuracy: 0.95643\n",
      "Epoch #51. Training Loss: 0.00008, Accuracy: 0.95663\n",
      "Epoch #52. Training Loss: 0.00008, Accuracy: 0.95701\n",
      "Epoch #53. Training Loss: 0.00008, Accuracy: 0.95728\n",
      "Epoch #54. Training Loss: 0.00008, Accuracy: 0.95586\n",
      "Epoch #55. Training Loss: 0.00008, Accuracy: 0.95606\n",
      "Epoch #56. Training Loss: 0.00008, Accuracy: 0.95658\n",
      "Epoch #57. Training Loss: 0.00008, Accuracy: 0.95686\n",
      "Epoch #58. Training Loss: 0.00008, Accuracy: 0.95718\n",
      "Epoch #59. Training Loss: 0.00008, Accuracy: 0.95720\n",
      "Epoch #60. Training Loss: 0.00008, Accuracy: 0.95732\n",
      "Epoch #61. Training Loss: 0.00008, Accuracy: 0.95715\n",
      "Epoch #62. Training Loss: 0.00008, Accuracy: 0.95691\n",
      "Epoch #63. Training Loss: 0.00008, Accuracy: 0.95697\n",
      "Epoch #64. Training Loss: 0.00008, Accuracy: 0.95571\n",
      "Epoch #65. Training Loss: 0.00008, Accuracy: 0.95689\n",
      "Epoch #66. Training Loss: 0.00008, Accuracy: 0.95554\n",
      "Epoch #67. Training Loss: 0.00008, Accuracy: 0.95698\n",
      "Epoch #68. Training Loss: 0.00008, Accuracy: 0.95675\n",
      "Epoch #69. Training Loss: 0.00008, Accuracy: 0.95732\n",
      "Epoch #70. Training Loss: 0.00008, Accuracy: 0.95665\n",
      "Epoch #71. Training Loss: 0.00008, Accuracy: 0.95717\n",
      "Epoch #72. Training Loss: 0.00008, Accuracy: 0.95660\n",
      "Epoch #73. Training Loss: 0.00008, Accuracy: 0.95681\n",
      "Epoch #74. Training Loss: 0.00008, Accuracy: 0.95636\n",
      "Epoch #75. Training Loss: 0.00008, Accuracy: 0.95655\n",
      "Epoch #76. Training Loss: 0.00008, Accuracy: 0.95662\n",
      "Epoch #77. Training Loss: 0.00008, Accuracy: 0.95677\n",
      "Epoch #78. Training Loss: 0.00008, Accuracy: 0.95704\n",
      "Epoch #79. Training Loss: 0.00008, Accuracy: 0.95697\n",
      "Epoch #80. Training Loss: 0.00008, Accuracy: 0.95681\n",
      "Epoch #81. Training Loss: 0.00008, Accuracy: 0.95668\n",
      "Epoch #82. Training Loss: 0.00008, Accuracy: 0.95603\n",
      "Epoch #83. Training Loss: 0.00008, Accuracy: 0.95651\n",
      "Epoch #84. Training Loss: 0.00008, Accuracy: 0.95673\n",
      "Epoch #85. Training Loss: 0.00008, Accuracy: 0.95686\n",
      "Epoch #86. Training Loss: 0.00008, Accuracy: 0.95677\n",
      "Epoch #87. Training Loss: 0.00008, Accuracy: 0.95711\n",
      "Epoch #88. Training Loss: 0.00008, Accuracy: 0.95528\n",
      "Epoch #89. Training Loss: 0.00008, Accuracy: 0.95599\n",
      "Epoch #90. Training Loss: 0.00008, Accuracy: 0.95702\n",
      "Epoch #91. Training Loss: 0.00008, Accuracy: 0.95722\n",
      "Epoch #92. Training Loss: 0.00008, Accuracy: 0.95700\n",
      "Epoch #93. Training Loss: 0.00008, Accuracy: 0.95744\n",
      "Epoch #94. Training Loss: 0.00008, Accuracy: 0.95757\n",
      "Epoch #95. Training Loss: 0.00008, Accuracy: 0.95652\n",
      "Epoch #96. Training Loss: 0.00008, Accuracy: 0.95597\n",
      "Epoch #97. Training Loss: 0.00008, Accuracy: 0.95672\n",
      "Epoch #98. Training Loss: 0.00008, Accuracy: 0.95680\n",
      "Epoch #99. Training Loss: 0.00008, Accuracy: 0.95555\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "def train_model(batch):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(batch.x, batch.edge_index)\n",
    "    loss = criterion(predictions, batch.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(), batch.x.size(0)\n",
    "\n",
    "def evaluate_model(batch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(batch.x, batch.edge_index)\n",
    "        pred_labels = predictions.argmax(dim=1)\n",
    "        correct_predictions = int((pred_labels == batch.y).sum())\n",
    "    return correct_predictions\n",
    "\n",
    "losses = []\n",
    "\n",
    "if gnnTrain:\n",
    "    loader = NeighborLoader(graph, num_neighbors=[-1, -1], batch_size=5000)\n",
    "    \n",
    "\n",
    "    for epoch in range(100):\n",
    "        total_loss = total_correct = total_nodes = 0\n",
    "\n",
    "        for batch in loader:\n",
    "            loss, nodes = train_model(batch)\n",
    "            total_loss += loss\n",
    "            total_nodes += nodes\n",
    "            total_correct += evaluate_model(batch)\n",
    "\n",
    "        average_loss = total_loss / total_nodes\n",
    "        accuracy = total_correct / total_nodes\n",
    "\n",
    "        losses.append(average_loss)\n",
    "        print(f\"Epoch #{epoch}. Training Loss: {average_loss:.5f}, Accuracy: {accuracy:.5f}\")\n",
    "        torch.save(model.state_dict(), gnn_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T20:00:36.563447Z",
     "iopub.status.busy": "2025-12-27T20:00:36.563268Z",
     "iopub.status.idle": "2025-12-27T20:00:37.397901Z",
     "shell.execute_reply": "2025-12-27T20:00:37.397231Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if create_store:\n",
    "    model.eval()\n",
    "    out = model.encode(graph.x, graph.edge_index).tolist()\n",
    "    \n",
    "    gnn_map = {}\n",
    "    \n",
    "    for i in range(len(mapp)):\n",
    "        gnn_map[lbl[mapp[i]]] = (out[i],list(nemap[mapp[i]]))\n",
    "    \n",
    "    # with open(\"data_files/emb_store.json\", \"w\") as file:\n",
    "    #     json.dump(gnn_map, file)\n",
    "        \n",
    "    with open(\"emb_store.json\", \"w\") as file:\n",
    "        json.dump(gnn_map, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T20:00:37.402788Z",
     "iopub.status.busy": "2025-12-27T20:00:37.402275Z",
     "iopub.status.idle": "2025-12-27T20:00:37.405028Z",
     "shell.execute_reply": "2025-12-27T20:00:37.404571Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with open(\"data_files/emb_store.json\", \"r\") as file:\n",
    "#     gnn_map = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T20:00:37.406480Z",
     "iopub.status.busy": "2025-12-27T20:00:37.406337Z",
     "iopub.status.idle": "2025-12-27T20:00:37.512706Z",
     "shell.execute_reply": "2025-12-27T20:00:37.512133Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"emb_store.json\", \"r\") as file:\n",
    "    gnn_map = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T20:00:37.516348Z",
     "iopub.status.busy": "2025-12-27T20:00:37.516179Z",
     "iopub.status.idle": "2025-12-27T20:00:37.521706Z",
     "shell.execute_reply": "2025-12-27T20:00:37.521144Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_features(filename=None, similarity=1):\n",
    "    nodes, y_train, edges, mapp, lbl, nemap = load_data(filename)\n",
    "    zero_vector = np.zeros(20)\n",
    "\n",
    "    X_train = []\n",
    "    for idx, map_item in enumerate(mapp):\n",
    "        label = lbl[map_item]\n",
    "        node_feature = nodes[idx]\n",
    "\n",
    "        if label in gnn_map:\n",
    "            emb, stored_set = gnn_map[label]\n",
    "            current_set = nemap[map_item]\n",
    "            \n",
    "            if len(current_set) == 0 and len(stored_set) == 0:\n",
    "                jaccard_similarity = 1.0  \n",
    "            elif len(current_set) == 0 or len(stored_set) == 0:\n",
    "                jaccard_similarity = 0.0  \n",
    "            else:\n",
    "                intersection = current_set.intersection(stored_set)\n",
    "                union = current_set.union(stored_set)\n",
    "                jaccard_similarity = len(intersection) / len(union)\n",
    "\n",
    "            feature_vector = emb if jaccard_similarity >= similarity else zero_vector\n",
    "        else:\n",
    "            feature_vector = zero_vector\n",
    "\n",
    "        X_train.append(np.hstack((node_feature, feature_vector)))\n",
    "\n",
    "    return np.array(X_train), y_train, edges, mapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T20:00:37.523226Z",
     "iopub.status.busy": "2025-12-27T20:00:37.523069Z",
     "iopub.status.idle": "2025-12-27T20:01:57.267248Z",
     "shell.execute_reply": "2025-12-27T20:01:57.266530Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72582, 8448, 6447, 4648, 3292, 2731, 2334, 2326, 2109, 2079, 1385, 1384, 1384, 1384, 1383, 1124, 1124, 1120, 1120, 1120, 1119, 1045, 1021, 1014, 893, 814, 808, 788, 767, 751, 717, 666, 650, 646, 643, 622, 607, 606, 606, 606, 606, 606, 606, 606, 606, 605, 604, 603, 603, 594, 556, 546, 534, 519, 519, 515, 498, 494, 488, 485, 480, 465, 451, 441, 441, 441, 441, 441, 441, 441, 441, 441, 441, 441, 441, 441, 440, 437, 419, 412, 411, 410, 406, 404, 396, 394, 388, 367, 365, 364, 361, 354, 354, 353, 333, 326, 313, 307, 291, 288, 280, 279, 276, 271, 271, 270, 264, 262, 256, 256, 255, 255, 255, 255, 255, 254, 254, 254, 254, 254, 254, 243, 237, 235, 230, 225, 225, 223, 221, 221, 220, 220, 219, 218, 217, 217, 215, 215, 215, 214, 213, 205, 205, 205, 203, 198, 198, 196, 196, 196, 196, 196, 196, 196, 196, 196, 194, 192, 191, 182, 182, 180, 176, 176, 174, 171, 170, 169, 168, 167, 166, 165, 165, 165, 164, 164, 161, 161, 159, 158, 154, 154, 152, 152, 151, 151, 151, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 149, 147, 146, 144, 144, 143, 142, 142, 141, 140, 140, 139, 139, 139, 138, 136, 135, 134, 134, 134, 134, 134, 134, 133, 133, 131, 131, 131, 130, 130, 129, 128, 128, 128, 128, 127, 127, 127, 127, 127, 125, 124, 124, 123, 123, 122, 121, 121, 121, 120, 119, 119, 119, 118, 118, 118, 118, 118, 118, 117, 117, 117, 117, 117, 117, 117, 116, 116, 116, 115, 113, 113, 112, 112, 111, 111, 110, 110, 109, 108, 108, 108, 107, 105, 105, 105, 104, 103, 103, 102, 102, 101, 101, 100, 100, 98, 98, 96, 96, 96, 95, 95, 94, 94, 94, 93, 93, 93, 93, 93, 93, 93, 93, 93, 92, 92, 92, 90, 90, 90, 90, 90, 89, 89, 89, 88, 88, 88, 87, 87, 85, 85, 85, 84, 84, 84, 84, 84, 84, 84, 83, 82, 82, 82, 81, 81, 81, 81, 81, 81, 81, 80, 80, 80, 79, 79, 79, 78, 78, 78, 78, 78, 78, 77, 77, 76, 76, 76, 75, 75, 75, 75, 75, 74, 74, 74, 74, 74, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 70, 70, 70, 69, 69, 69, 69, 69, 68, 68, 68, 68, 68, 68, 68, 67, 67, 67, 67, 67, 67, 67, 67, 67, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]\n",
      "Sucessfully add edges: 194641\tPrune edges: 0\tFail to add edges: 143231\n",
      "0.9968841992583247\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "import xgboost as xgb\n",
    "\n",
    "if xgbTrain:\n",
    "    file_path = '../SysClient0201.systemia.com.txt'\n",
    "    x,y,_,_ = load_features(file_path)\n",
    "    \n",
    "\n",
    "    xgb_cl = xgb.XGBClassifier()\n",
    "\n",
    "    xgb_cl.fit(x,y)\n",
    "    pickle.dump(xgb_cl, open(xgboost_weights, \"wb\"))\n",
    "\n",
    "    preds = xgb_cl.predict(x)\n",
    "    print(accuracy_score(y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T20:01:57.270788Z",
     "iopub.status.busy": "2025-12-27T20:01:57.270521Z",
     "iopub.status.idle": "2025-12-27T20:01:57.273725Z",
     "shell.execute_reply": "2025-12-27T20:01:57.273304Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_pkl(fname):\n",
    "    with open(fname, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T20:01:57.275365Z",
     "iopub.status.busy": "2025-12-27T20:01:57.275044Z",
     "iopub.status.idle": "2025-12-27T20:01:57.278714Z",
     "shell.execute_reply": "2025-12-27T20:01:57.278294Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate(file_path):\n",
    "    x,y,_,_ = load_features(file_path)\n",
    "    xgb_cl = load_pkl(xgboost_weights)\n",
    "\n",
    "    pred = xgb_cl.predict(x)\n",
    "    proba = xgb_cl.predict_proba(x)\n",
    "\n",
    "    sorted = np.sort(proba, axis=1)\n",
    "    conf = (sorted[:,-1] - sorted[:,-2]) / sorted[:,-1]\n",
    "    conf = (conf - conf.min()) / conf.max()\n",
    "\n",
    "    check = (pred == y)\n",
    "    flag = ~torch.tensor(check)\n",
    "    scores = conf[flag].tolist()\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T20:01:57.280362Z",
     "iopub.status.busy": "2025-12-27T20:01:57.280010Z",
     "iopub.status.idle": "2025-12-27T20:01:57.318193Z",
     "shell.execute_reply": "2025-12-27T20:01:57.317449Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import compress\n",
    "from torch_geometric import utils\n",
    "\n",
    "def Get_Adjacent(ids, mapp, edges, hops):\n",
    "    if hops == 0:\n",
    "        return set()\n",
    "    \n",
    "    neighbors = set()\n",
    "    for edge in zip(edges[0], edges[1]):\n",
    "        if any(mapp[node] in ids for node in edge):\n",
    "            neighbors.update(mapp[node] for node in edge)\n",
    "\n",
    "    if hops > 1:\n",
    "        neighbors = neighbors.union(Get_Adjacent(neighbors, mapp, edges, hops - 1))\n",
    "    \n",
    "    return neighbors\n",
    "\n",
    "def calculate_metrics(TP, FP, FN, TN):\n",
    "    FPR = FP / (FP + TN) if FP + TN > 0 else 0\n",
    "    TPR = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "\n",
    "    prec = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    rec = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    fscore = (2 * prec * rec) / (prec + rec) if prec + rec > 0 else 0\n",
    "\n",
    "    return prec, rec, fscore, FPR, TPR\n",
    "\n",
    "def helper(MP, all_pids, GP, edges, mapp):\n",
    "    TP = MP.intersection(GP)\n",
    "    FP = MP - GP\n",
    "    FN = GP - MP\n",
    "    TN = all_pids - (GP | MP)\n",
    "\n",
    "    two_hop_gp = Get_Adjacent(GP, mapp, edges, 2)\n",
    "    two_hop_tp = Get_Adjacent(TP, mapp, edges, 2)\n",
    "    FPL = FP - two_hop_gp\n",
    "    TPL = TP.union(FN.intersection(two_hop_tp))\n",
    "    FN = FN - two_hop_tp\n",
    "\n",
    "    TP, FP, FN, TN = len(TPL), len(FPL), len(FN), len(TN)\n",
    "\n",
    "    prec, rec, fscore, FPR, TPR = calculate_metrics(TP, FP, FN, TN)\n",
    "    print(f\"True Positives: {TP}, True Negatives: {TN}, False Positives: {FP}, False Negatives: {FN}\")\n",
    "    print(f\"Precision: {round(prec, 2)}, Recall: {round(rec, 2)}, Fscore: {round(fscore, 2)}\")\n",
    "    \n",
    "    return TPL, FPL\n",
    "\n",
    "def calculate_similarity(set1, set2):\n",
    "    if len(set1) == 0 and len(set2) == 0:\n",
    "        return 1.0  \n",
    "    elif len(set1) == 0 or len(set2) == 0:\n",
    "        return 0.0  \n",
    "    \n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    return len(intersection) / len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T20:01:57.320015Z",
     "iopub.status.busy": "2025-12-27T20:01:57.319796Z",
     "iopub.status.idle": "2025-12-27T20:01:57.324007Z",
     "shell.execute_reply": "2025-12-27T20:01:57.323522Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_features_test(dataframe, similarity_threshold=1):\n",
    "    nodes, y_train, edges, mapping, label_map, node_entity_map = Featurize(dataframe, csv_path)\n",
    "    X_train = []\n",
    "\n",
    "    for i, map_id in enumerate(mapping):\n",
    "        label = label_map[map_id]\n",
    "        node_embedding = np.zeros(20)  \n",
    "\n",
    "        if label in gnn_map:\n",
    "            embedding, stored_set = gnn_map[label]\n",
    "            current_set = node_entity_map[map_id]\n",
    "            similarity_metric = calculate_similarity(current_set, stored_set)\n",
    "\n",
    "            if similarity_metric >= similarity_threshold:\n",
    "                node_embedding = np.array(embedding)\n",
    "\n",
    "        X_train.append(np.hstack((nodes[i], node_embedding)))\n",
    "\n",
    "    return np.array(X_train), y_train, edges, mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T20:01:57.325430Z",
     "iopub.status.busy": "2025-12-27T20:01:57.325269Z",
     "iopub.status.idle": "2025-12-27T20:01:57.327644Z",
     "shell.execute_reply": "2025-12-27T20:01:57.327262Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T20:01:57.329146Z",
     "iopub.status.busy": "2025-12-27T20:01:57.328883Z",
     "iopub.status.idle": "2025-12-27T20:01:57.334073Z",
     "shell.execute_reply": "2025-12-27T20:01:57.333647Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_events_from_hosts(hosts):\n",
    "    all_events = []\n",
    "    for host in hosts:\n",
    "        path = f'../SysClient0{host}.systemia.com.txt'\n",
    "        with open(path, 'r') as file:\n",
    "            raw_events = [json.loads(line) for line in file]\n",
    "        all_events.extend(raw_events)\n",
    "    return all_events\n",
    "\n",
    "def load_ground_truth(gt_file):\n",
    "    with open(gt_file, 'r') as file:\n",
    "        gt_nodes = set(file.read().split())\n",
    "    return gt_nodes\n",
    "\n",
    "def evaluate_model(df, xgb_cl, similarity_threshold, confidence_threshold):\n",
    "    x, y, edges, mapp = load_features_test(df)\n",
    "\n",
    "    pred = xgb_cl.predict(x)\n",
    "    proba = xgb_cl.predict_proba(x)\n",
    "\n",
    "    sorted_proba = np.sort(proba, axis=1)\n",
    "    conf = (sorted_proba[:, -1] - sorted_proba[:, -2]) / sorted_proba[:, -1]\n",
    "    normalized_conf = (conf - conf.min()) / conf.max()\n",
    "\n",
    "    check = (pred == y) & (normalized_conf > confidence_threshold)\n",
    "    flag = ~torch.tensor(check)\n",
    "\n",
    "    index = utils.mask_to_index(flag).tolist()\n",
    "    ids = {mapp[idx] for idx in index}\n",
    "    return ids,edges,mapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T20:01:57.335507Z",
     "iopub.status.busy": "2025-12-27T20:01:57.335354Z",
     "iopub.status.idle": "2025-12-27T20:01:57.343213Z",
     "shell.execute_reply": "2025-12-27T20:01:57.342778Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def read_event_data(host):\n",
    "    file_path = f'../SysClient0{host}.systemia.com.txt'\n",
    "    with open(file_path, 'r') as file:\n",
    "        return [json.loads(line) for line in file]\n",
    "        \n",
    "def stream_events(batch_size, window_size):\n",
    "    event_buffer = {}\n",
    "    hosts = ['051']\n",
    "    positions = {host: 0 for host in hosts}\n",
    "    while True:\n",
    "        for host in hosts:\n",
    "            if host not in event_buffer or len(event_buffer[host]) < positions[host] + batch_size:\n",
    "                events = read_event_data(host)\n",
    "                dframe = transform(events)\n",
    "                if host in event_buffer:\n",
    "                    event_buffer[host] = event_buffer[host].append(dframe, ignore_index=True)\n",
    "                else:\n",
    "                    event_buffer[host] = dframe\n",
    "            start = positions[host]\n",
    "            end = start + batch_size\n",
    "            yield event_buffer[host][start:end]\n",
    "            positions[host] += window_size\n",
    "            if positions[host] >= len(event_buffer[host]):\n",
    "                return\n",
    "\n",
    "def analyze_events(data_frame, ground_truth_nodes):\n",
    "    \n",
    "    if data_frame['properties'].apply(lambda x: isinstance(x, str)).any():\n",
    "        data_frame['properties'] = data_frame['properties'].apply(json.loads)\n",
    "        \n",
    "    actor_and_object_ids = set(data_frame['actorID']) | set(data_frame['objectID'])\n",
    "    relevant_ground_truth = {x for x in ground_truth_nodes if x in actor_and_object_ids}\n",
    "\n",
    "    features, labels, edges, mapping = load_features_test(data_frame)\n",
    "    model = load_pkl(xgboost_weights)\n",
    "\n",
    "    predictions = model.predict(features)\n",
    "    probabilities = model.predict_proba(features)\n",
    "\n",
    "    sorted_probabilities = np.sort(probabilities, axis=1)\n",
    "    confidence_scores = (sorted_probabilities[:, -1] - sorted_probabilities[:, -2]) / sorted_probabilities[:, -1]\n",
    "    normalized_confidence = (confidence_scores - confidence_scores.min()) / confidence_scores.max()\n",
    "\n",
    "    misclassified = ~torch.tensor(predictions == labels)\n",
    "    misclassified_indices = utils.mask_to_index(misclassified).tolist()\n",
    "    misclassified_ids = {mapping[idx] for idx in misclassified_indices}\n",
    "\n",
    "    helper(misclassified_ids, actor_and_object_ids, relevant_ground_truth, edges, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T20:01:57.344882Z",
     "iopub.status.busy": "2025-12-27T20:01:57.344556Z",
     "iopub.status.idle": "2025-12-27T20:01:57.349959Z",
     "shell.execute_reply": "2025-12-27T20:01:57.349503Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def traverse(ids, mapping, edges, hops, visited=None):\n",
    "    if hops == 0:\n",
    "        return set()\n",
    "\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "\n",
    "    neighbors = set()\n",
    "    for src, dst in zip(edges[0], edges[1]):\n",
    "        src_mapped, dst_mapped = mapping[src], mapping[dst]\n",
    "\n",
    "        if (src_mapped in ids and dst_mapped not in visited) or \\\n",
    "           (dst_mapped in ids and src_mapped not in visited):\n",
    "            neighbors.add(src_mapped)\n",
    "            neighbors.add(dst_mapped)\n",
    "\n",
    "        visited.add(src_mapped)\n",
    "        visited.add(dst_mapped)\n",
    "\n",
    "    neighbors.difference_update(ids) \n",
    "    return ids.union(traverse(neighbors, mapping, edges, hops - 1, visited))\n",
    "\n",
    "# def load_data(file_path):\n",
    "#     with open(file_path, 'r') as file:\n",
    "#         return json.load(file)\n",
    "\n",
    "def find_connected_alerts(start_alert, mapping, edges, depth, remaining_alerts):\n",
    "    connected_path = traverse({start_alert}, mapping, edges, depth)\n",
    "    return connected_path.intersection(remaining_alerts)\n",
    "\n",
    "def generate_incident_graphs(alerts, edges, mapping, depth):\n",
    "    incident_graphs = []\n",
    "    remaining_alerts = set(alerts)\n",
    "\n",
    "    while remaining_alerts:\n",
    "        alert = remaining_alerts.pop()\n",
    "        connected_alerts = find_connected_alerts(alert, mapping, edges, depth, remaining_alerts)\n",
    "\n",
    "        if len(connected_alerts) > 1:\n",
    "            incident_graphs.append(connected_alerts)\n",
    "            remaining_alerts -= connected_alerts\n",
    "\n",
    "    return incident_graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Testing Flash on OpTC Malicious Upgrade Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T20:01:57.351645Z",
     "iopub.status.busy": "2025-12-27T20:01:57.351299Z",
     "iopub.status.idle": "2025-12-27T20:01:57.353621Z",
     "shell.execute_reply": "2025-12-27T20:01:57.353221Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# all_events = load_events_from_hosts(['051'])\n",
    "\n",
    "# EnActIds = [x['actorID'] for x in all_events]\n",
    "# EnObjIds = [x['objectID'] for x in all_events]\n",
    "# EntitySet = set(EnActIds).union(set(EnObjIds))\n",
    "\n",
    "# df = transform(all_events)\n",
    "\n",
    "# gt_nodes = load_ground_truth('optc.txt')\n",
    "# gt_nodes = [x for x in gt_nodes if x in EntitySet]\n",
    "# gt_nodes = set(gt_nodes)\n",
    "\n",
    "# xgboost_model = load_pkl(xgboost_weights)\n",
    "# identified_ids,edges,mapp = evaluate_model(df, xgboost_model, 1, 0.6)\n",
    "\n",
    "# alerts = helper(identified_ids, EntitySet, gt_nodes, edges, mapp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Flash on OpTC Plain PowerShell Empire Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T20:01:57.355192Z",
     "iopub.status.busy": "2025-12-27T20:01:57.354892Z",
     "iopub.status.idle": "2025-12-27T20:02:40.520597Z",
     "shell.execute_reply": "2025-12-27T20:02:40.519688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72582, 8448, 6447, 4648, 3292, 2731, 2334, 2326, 2109, 2079, 1385, 1384, 1384, 1384, 1383, 1124, 1124, 1120, 1120, 1120, 1119, 1045, 1021, 1014, 893, 814, 808, 788, 767, 751, 717, 666, 650, 646, 643, 622, 607, 606, 606, 606, 606, 606, 606, 606, 606, 605, 604, 603, 603, 594, 556, 546, 534, 519, 519, 515, 498, 494, 488, 485, 480, 465, 451, 441, 441, 441, 441, 441, 441, 441, 441, 441, 441, 441, 441, 441, 440, 437, 419, 412, 411, 410, 406, 404, 396, 394, 388, 367, 365, 364, 361, 354, 354, 353, 333, 326, 313, 307, 291, 288, 280, 279, 276, 271, 271, 270, 264, 262, 256, 256, 255, 255, 255, 255, 255, 254, 254, 254, 254, 254, 254, 243, 237, 235, 230, 225, 225, 223, 221, 221, 220, 220, 219, 218, 217, 217, 215, 215, 215, 214, 213, 205, 205, 205, 203, 198, 198, 196, 196, 196, 196, 196, 196, 196, 196, 196, 194, 192, 191, 182, 182, 180, 176, 176, 174, 171, 170, 169, 168, 167, 166, 165, 165, 165, 164, 164, 161, 161, 159, 158, 154, 154, 152, 152, 151, 151, 151, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 149, 147, 146, 144, 144, 143, 142, 142, 141, 140, 140, 139, 139, 139, 138, 136, 135, 134, 134, 134, 134, 134, 134, 133, 133, 131, 131, 131, 130, 130, 129, 128, 128, 128, 128, 127, 127, 127, 127, 127, 125, 124, 124, 123, 123, 122, 121, 121, 121, 120, 119, 119, 119, 118, 118, 118, 118, 118, 118, 117, 117, 117, 117, 117, 117, 117, 116, 116, 116, 115, 113, 113, 112, 112, 111, 111, 110, 110, 109, 108, 108, 108, 107, 105, 105, 105, 104, 103, 103, 102, 102, 101, 101, 100, 100, 98, 98, 96, 96, 96, 95, 95, 94, 94, 94, 93, 93, 93, 93, 93, 93, 93, 93, 93, 92, 92, 92, 90, 90, 90, 90, 90, 89, 89, 89, 88, 88, 88, 87, 87, 85, 85, 85, 84, 84, 84, 84, 84, 84, 84, 83, 82, 82, 82, 81, 81, 81, 81, 81, 81, 81, 80, 80, 80, 79, 79, 79, 78, 78, 78, 78, 78, 78, 77, 77, 76, 76, 76, 75, 75, 75, 75, 75, 74, 74, 74, 74, 74, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 70, 70, 70, 69, 69, 69, 69, 69, 68, 68, 68, 68, 68, 68, 68, 67, 67, 67, 67, 67, 67, 67, 67, 67, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]\n",
      "Sucessfully add edges: 194641\tPrune edges: 0\tFail to add edges: 143231\n",
      "True Positives: 62, True Negatives: 204797, False Positives: 0, False Negatives: 0\n",
      "Precision: 1.0, Recall: 1.0, Fscore: 1.0\n"
     ]
    }
   ],
   "source": [
    "all_events = load_events_from_hosts(['201'])\n",
    "\n",
    "EnActIds = [x['actorID'] for x in all_events]\n",
    "EnObjIds = [x['objectID'] for x in all_events]\n",
    "EntitySet = set(EnActIds).union(set(EnObjIds))\n",
    "\n",
    "df = transform(all_events)\n",
    "\n",
    "gt_nodes = load_ground_truth('../optc.txt')\n",
    "gt_nodes = [x for x in gt_nodes if x in EntitySet]\n",
    "gt_nodes = set(gt_nodes)\n",
    "\n",
    "xgboost_model = load_pkl(xgboost_weights)\n",
    "identified_ids,edges,mapp = evaluate_model(df, xgboost_model, 1, 0)\n",
    "\n",
    "alerts = helper(identified_ids, EntitySet, gt_nodes, edges, mapp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Flash on OpTC Custom PowerShell Empire Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T20:02:40.523393Z",
     "iopub.status.busy": "2025-12-27T20:02:40.523217Z",
     "iopub.status.idle": "2025-12-27T20:02:40.526086Z",
     "shell.execute_reply": "2025-12-27T20:02:40.525687Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# all_events = load_events_from_hosts(['501'])\n",
    "\n",
    "# EnActIds = [x['actorID'] for x in all_events]\n",
    "# EnObjIds = [x['objectID'] for x in all_events]\n",
    "# EntitySet = set(EnActIds).union(set(EnObjIds))\n",
    "\n",
    "# df = transform(all_events)\n",
    "\n",
    "# gt_nodes = load_ground_truth('optc.txt')\n",
    "# gt_nodes = [x for x in gt_nodes if x in EntitySet]\n",
    "# gt_nodes = set(gt_nodes)\n",
    "\n",
    "# xgboost_model = load_pkl(xgboost_weights)\n",
    "# identified_ids,edges,mapp = evaluate_model(df, xgboost_model, 1, 0.98)\n",
    "\n",
    "# alerts = helper(identified_ids, EntitySet, gt_nodes, edges, mapp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Flash on Streaming Batches Generated from OpTC Attack Logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T20:02:40.527571Z",
     "iopub.status.busy": "2025-12-27T20:02:40.527421Z",
     "iopub.status.idle": "2025-12-27T20:02:40.530113Z",
     "shell.execute_reply": "2025-12-27T20:02:40.529707Z"
    }
   },
   "outputs": [],
   "source": [
    "stream = False\n",
    "if stream:\n",
    "    for data_frame in stream_events(250000, 250):\n",
    "        gt_nodes = load_ground_truth('optc.txt')\n",
    "        analyze_events(data_frame, gt_nodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flash-wyj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
